# -*- coding: utf-8 -*-
"""
æœ¬ç¨‹å¼ç¢¼ç‚º Million Song Dataset (MSD) çš„è³‡æ–™å‰è™•ç†éšæ®µï¼Œ
åŒ…å«ä»¥ä¸‹ä»»å‹™ï¼š
1. è¼‰å…¥åŸå§‹ä¸‰å…ƒçµ„è³‡æ–™ï¼ˆuser, song, play_countï¼‰
2. çµ±è¨ˆæ¯ä½ä½¿ç”¨è€…èˆ‡æ¯é¦–æ­Œæ›²çš„ç¸½æ’­æ”¾é‡ï¼Œä¸¦å„è‡ªå„²å­˜ç‚º CSV
3. ç¯©é¸å‰ 10 è¬åæ´»èºç”¨æˆ¶èˆ‡å‰ 3 è¬é¦–ç†±é–€æ­Œæ›²
4. æ“·å–åŒ…å«ä¸Šè¿°å­é›†çš„è³‡æ–™åˆ—ï¼Œå¦å­˜ç‚º subset è³‡æ–™é›†
5. å°‡æ­Œæ›²ä¸­å°æ‡‰çš„ metadataï¼ˆæ­Œæ‰‹ã€æ›²åã€å°ˆè¼¯ç­‰ï¼‰åˆä½µ
6. ç§»é™¤å†—é¤˜æ¬„ä½ï¼Œè¼¸å‡ºä¹¾æ·¨åˆä½µå¾Œçš„è³‡æ–™é›†
"""
import pandas as pd
import numpy as np
import os
import sqlite3
import requests
import zipfile
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")



def download_file_with_progress(url, save_path):
    """
    ä½¿ç”¨ requests + tqdm é¡¯ç¤ºé€²åº¦æ¢ä¸‹è¼‰æª”æ¡ˆ
    """
    if os.path.exists(save_path):
        print(f"æª”æ¡ˆå·²å­˜åœ¨ï¼š{save_path}")
        return

    try:
        print(f"é–‹å§‹ä¸‹è¼‰ï¼š{url}")
        with requests.get(url, stream=True, verify=False) as r:
            r.raise_for_status()  # å¦‚æœ HTTP éŒ¯èª¤æœƒ raise Exception
            total = int(r.headers.get('content-length', 0))
            with open(save_path, 'wb') as f, tqdm(
                desc=save_path,
                total=total,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
                ncols=80
            ) as bar:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        bar.update(len(chunk))
        print(f"å·²ä¸‹è¼‰åˆ°ï¼š{save_path}")
    except Exception as e:
        print(f"ä¸‹è¼‰å¤±æ•—ï¼š{e}")


def compute_user_play_counts(filepath):
    """
    çµ±è¨ˆæ¯ä½ä½¿ç”¨è€…çš„æ’­æ”¾ç¸½æ¬¡æ•¸
    è¼¸å…¥: ä½¿ç”¨è€…-æ­Œæ›²-æ’­æ”¾é‡çš„æª”æ¡ˆè·¯å¾‘
    è¼¸å‡º: DataFrame åŒ…å« 'user' èˆ‡å…¶å°æ‡‰çš„ 'play_count'
    """
    output_dict = {}
    with open(filepath) as f:
        for line in f:
            user = line.split('\t')[0]
            play_count = int(line.split('\t')[2])
            if user in output_dict:
                play_count += output_dict[user]
            output_dict[user] = play_count
    user_play_df = pd.DataFrame([{'user': k, 'play_count': v} for k, v in output_dict.items()])
    user_play_df = user_play_df.sort_values(by='play_count', ascending=False)
    return user_play_df


def compute_song_play_counts(filepath):
    """
    çµ±è¨ˆæ¯é¦–æ­Œæ›²çš„æ’­æ”¾ç¸½æ¬¡æ•¸
    è¼¸å…¥: ä½¿ç”¨è€…-æ­Œæ›²-æ’­æ”¾é‡çš„æª”æ¡ˆè·¯å¾‘
    è¼¸å‡º: DataFrame åŒ…å« 'song' èˆ‡å…¶å°æ‡‰çš„ 'play_count'
    """
    output_dict = {}
    with open(filepath) as f:
        for line in f:
            song = line.split('\t')[1]
            play_count = int(line.split('\t')[2])
            if song in output_dict:
                play_count += output_dict[song]
            output_dict[song] = play_count
    song_play_df = pd.DataFrame([{'song': k, 'play_count': v} for k, v in output_dict.items()])
    song_play_df = song_play_df.sort_values(by='play_count', ascending=False)
    return song_play_df



if __name__ == "__main__":


    # ===== 0. ä¸‹è¼‰è³‡æ–™é›† =====
    # ä¸‹è¼‰ track_metadata.db
    download_file_with_progress(
        "https://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db",
        "RecommendationSystem/track_metadata.db"
    )

    # ä¸‹è¼‰ä¸¦è§£å£“ train_triplets.txt.zip
    zip_path = "RecommendationSystem/train_triplets.txt.zip"
    txt_path = "RecommendationSystem/train_triplets.txt"

    download_file_with_progress(
        "http://millionsongdataset.com/sites/default/files/challenge/train_triplets.txt.zip",
        zip_path
    )

    if not os.path.exists(txt_path) and os.path.exists(zip_path):
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall("RecommendationSystem/")
            print("train_triplets.txt å·²æˆåŠŸè§£å£“")
        except Exception as e:
            print(f"ğŸš« è§£å£“ç¸®éŒ¯èª¤ï¼š{e}")
    else:
        print("train_triplets.txt å·²å­˜åœ¨ï¼Œç•¥éè§£å£“")

    # ===== 1. è®€å–åŸå§‹ä½¿ç”¨è€…-æ­Œæ›²-æ’­æ”¾é‡è³‡æ–™ =====
    
    # è¨­å®šè³‡æ–™ç›®éŒ„è·¯å¾‘
    data_home = './RecommendationSystem/'

    # è®€å–ä¸‰æ¬„æ ¼å¼çš„ä½¿ç”¨è€…æ’­æ”¾è³‡æ–™ï¼šuser_id, song_id, play_count
    triplet_dataset = pd.read_csv(f'{data_home}train_triplets.txt',
                                sep='\t', header=None,
                                names=['user', 'song', 'play_count'])
    
    print("åŸå§‹è³‡æ–™ç­†æ•¸èˆ‡æ¬„ä½æ•¸:", triplet_dataset.shape)
    print("è³‡æ–™çµæ§‹æ‘˜è¦:")
    print(triplet_dataset.info())
    print("å‰ 10 ç­†åŸå§‹æ’­æ”¾ç´€éŒ„:")
    print(triplet_dataset.head(10))

    # ===== 2. çµ±è¨ˆæ¯ä½ä½¿ç”¨è€…çš„ç¸½æ’­æ”¾æ¬¡æ•¸ =====
    # å¦‚æœ user_playcount_df.csv å·²å­˜åœ¨ï¼Œå‰‡ä¸å†é‡è¤‡è¨ˆç®—
    if os.path.exists(f'{data_home}user_playcount_df.csv'):
        play_count_df = pd.read_csv(f'{data_home}user_playcount_df.csv')
    else:
        play_count_df = compute_user_play_counts(f'{data_home}train_triplets.txt')
        play_count_df.to_csv(f'{data_home}user_playcount_df.csv', index=False)

    print("æ’­æ”¾æ¬¡æ•¸æœ€å¤šçš„å‰ 10 åä½¿ç”¨è€…:")
    print(play_count_df.head(10))

    # ===== 3. çµ±è¨ˆæ¯é¦–æ­Œæ›²çš„ç¸½æ’­æ”¾æ¬¡æ•¸ =====
    # å¦‚æœ song_playcount_df.csv å·²å­˜åœ¨ï¼Œå‰‡ä¸å†é‡è¤‡è¨ˆç®—
    if os.path.exists(f'{data_home}song_playcount_df.csv'):
        song_count_df = pd.read_csv(f'{data_home}song_playcount_df.csv')
    else:
        song_count_df = compute_song_play_counts(f'{data_home}train_triplets.txt')
        song_count_df.to_csv(f'{data_home}song_playcount_df.csv', index=False)

    print("æ’­æ”¾æ¬¡æ•¸æœ€å¤šçš„å‰ 10 é¦–æ­Œæ›²:")
    print(song_count_df.head(10))

    # ===== 4. æ“·å–å‰ 10 è¬æ´»èºç”¨æˆ¶èˆ‡å‰ 3 è¬ç†±é–€æ­Œæ›²ä½œç‚ºå­é›† =====

    total_play_count = song_count_df['play_count'].sum()
    
    print("å‰ 10 è¬åä½¿ç”¨è€…ä½”ç¸½æ’­æ”¾æ¯”ä¾‹ (%):",(play_count_df.head(100000)['play_count'].sum() / total_play_count) * 100)
    
    print("å‰ 3 è¬é¦–æ­Œæ›²ä½”ç¸½æ’­æ”¾æ¯”ä¾‹ (%):",(song_count_df.head(30000)['play_count'].sum() / total_play_count) * 100)

    user_subset = list(play_count_df.head(100000)['user'])
    song_subset = list(song_count_df.head(30000)['song'])

    # ===== 5. éæ¿¾åŸå§‹è³‡æ–™ï¼Œåªä¿ç•™ subset ä¸­çš„ä½¿ç”¨è€…èˆ‡æ­Œæ›² =====
    triplet_dataset = pd.read_csv(f'{data_home}train_triplets.txt', sep='\t',
                                header=None, names=['user', 'song', 'play_count'])
    triplet_dataset_sub = triplet_dataset[triplet_dataset['user'].isin(user_subset)]
    triplet_dataset_sub_song = triplet_dataset_sub[triplet_dataset_sub['song'].isin(song_subset)]
    # å¦‚æœ triplet_dataset_sub_song.csv å·²å­˜åœ¨ï¼Œå‰‡ä¸å†é‡è¤‡è¨ˆç®—
    if not os.path.exists(f'{data_home}triplet_dataset_sub_song.csv'):
        triplet_dataset_sub_song.to_csv(f'{data_home}triplet_dataset_sub_song.csv', index=False)

    print("éæ¿¾å¾Œçš„æ’­æ”¾ç´€éŒ„ç­†æ•¸èˆ‡æ¬„ä½æ•¸:", triplet_dataset_sub_song.shape)
    print("éæ¿¾å¾Œçš„æ’­æ”¾è³‡æ–™å‰ 10 ç­†:")
    print(triplet_dataset_sub_song.head(10))

    # ===== 6. åˆä½µæ­Œæ›²çš„ metadata è³‡æ–™ =====
    conn = sqlite3.connect(f'{data_home}track_metadata.db')
    track_metadata_df = pd.read_sql('SELECT * FROM songs', con=conn)
    track_metadata_df_sub = track_metadata_df[track_metadata_df['song_id'].isin(song_subset)]
    # å¦‚æœ track_metadata_df_sub.csv å·²å­˜åœ¨ï¼Œå‰‡ä¸å†é‡è¤‡è¨ˆç®—
    if not os.path.exists(f'{data_home}track_metadata_df_sub.csv'):
        track_metadata_df_sub.to_csv(f'{data_home}track_metadata_df_sub.csv', index=False)
    
    print("æ­Œæ›² metadata å­é›†ç­†æ•¸èˆ‡æ¬„ä½æ•¸:", track_metadata_df_sub.shape)

    # ===== 7. æ¸…ç†èˆ‡åˆä½µè³‡æ–™ =====
    track_metadata_df_sub.drop(columns=['track_id', 'artist_mbid'], inplace=True)
    track_metadata_df_sub = track_metadata_df_sub.drop_duplicates(subset=['song_id'])

    print("æ’­æ”¾è³‡æ–™é›† (åˆä½µå‰) å‰å¹¾ç­†:")
    print(triplet_dataset_sub_song.head())
    print("æ­Œæ›²ä¸­ç¹¼è³‡æ–™é›† (åˆä½µå‰) å‰å¹¾ç­†:")
    print(track_metadata_df_sub.head())

    # åˆä½µæ’­æ”¾è³‡æ–™èˆ‡ metadata
    triplet_dataset_sub_song_merged = pd.merge(triplet_dataset_sub_song,track_metadata_df_sub,how='left', left_on='song', right_on='song_id')
    triplet_dataset_sub_song_merged.rename(columns={'play_count': 'listen_count'}, inplace=True)

    # å»é™¤ä¸å¿…è¦æ¬„ä½
    cols_to_drop = ['song_id', 'artist_id', 'duration',
                    'artist_familiarity', 'artist_hotttnesss',
                    'track_7digitalid', 'shs_perf', 'shs_work']
    triplet_dataset_sub_song_merged.drop(columns=cols_to_drop, inplace=True)

    # å¦‚æœ triplet_dataset_sub_song_merged.csv å·²å­˜åœ¨ï¼Œå‰‡ä¸å†é‡è¤‡å­˜æª”
    if not os.path.exists(f'{data_home}triplet_dataset_sub_song_merged.csv'):
        triplet_dataset_sub_song_merged.to_csv(f'{data_home}triplet_dataset_sub_song_merged.csv', index=False)

    # æœ€çµ‚è™•ç†å¾Œè³‡æ–™
    print("æ•´åˆå¾Œçš„è³‡æ–™å‰ 10 ç­†:")
    print(triplet_dataset_sub_song_merged.head(10))