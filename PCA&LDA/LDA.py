# -*- coding: utf-8 -*-
"""
ğŸ“˜ æœ¬ç¨‹å¼èªªæ˜ï¼š
æœ¬ç¨‹å¼ç¤ºç¯„å¦‚ä½•ä»¥ LDAï¼ˆLinear Discriminant Analysisï¼‰å¯¦ä½œé™ç¶­æµç¨‹ï¼Œä½¿ç”¨ iris é³¶å°¾èŠ±è³‡æ–™é›†ã€‚
éç¨‹åŒ…å«ï¼š
1. è³‡æ–™é è™•ç†èˆ‡æ¨™æº–åŒ–
2. é¡å…§/é¡é–“æ•£åº¦çŸ©é™£è¨ˆç®—
3. å»£ç¾©ç‰¹å¾µå€¼åˆ†è§£
4. æ‰‹å‹•èˆ‡ sklearn å¯¦ä½œçš„ LDA æ¯”è¼ƒ
5. åŸå§‹ç©ºé–“èˆ‡é™ç¶­å¾Œç©ºé–“çš„è¦–è¦ºåŒ–å‘ˆç¾
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rc('font', family='Microsoft JhengHei')  # è¨­å®šä¸­æ–‡å­—å‹ç‚ºå¾®è»Ÿæ­£é»‘é«”
plt.rcParams['axes.unicode_minus'] = False   # æ­£ç¢ºé¡¯ç¤ºè² è™Ÿ
from sklearn.preprocessing import LabelEncoder
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA


def plot_step_lda():
    """
    å‡½æ•¸åŠŸèƒ½ï¼šè¦–è¦ºåŒ–åŸå§‹è³‡æ–™åœ¨ X[0]-X[1] å¹³é¢ä¸Šçš„åˆ†å¸ƒ
    è¼¸å…¥ï¼šç„¡ï¼ˆç›´æ¥ä½¿ç”¨å…¨åŸŸè®Šæ•¸ X, yï¼‰
    è¼¸å‡ºï¼šmatplotlib è¦–è¦ºåŒ–åœ–å½¢
    """
    ax = plt.subplot(111)
    for label, marker, color in zip(range(1, 4), ('^', 's', 'o'), ('blue', 'red', 'green')):
        plt.scatter(X[y == label, 0], X[y == label, 1],
                    marker=marker, color=color, alpha=0.5,
                    label=label_dict[label])
    plt.xlabel('X[0]')
    plt.ylabel('X[1]')
    plt.title('åŸå§‹è³‡æ–™ç©ºé–“')
    plt.legend(loc='upper right', fancybox=True).get_frame().set_alpha(0.5)
    plt.grid()
    plt.tight_layout()
    plt.show()


def plot_step_lda2():
    """
    å‡½æ•¸åŠŸèƒ½ï¼šè¦–è¦ºåŒ–é™ç¶­å¾Œ LDA ç©ºé–“çš„åˆ†å¸ƒï¼ˆLD1 vs LD2ï¼‰
    è¼¸å…¥ï¼šç„¡ï¼ˆä½¿ç”¨å…¨åŸŸè®Šæ•¸ X_lda, yï¼‰
    è¼¸å‡ºï¼šmatplotlib è¦–è¦ºåŒ–åœ–å½¢
    """
    ax = plt.subplot(111)
    for label, marker, color in zip(range(1, 4), ('^', 's', 'o'), ('blue', 'red', 'green')):
        plt.scatter(X_lda[y == label, 0], X_lda[y == label, 1],
                    marker=marker, color=color, alpha=0.5,
                    label=label_dict[label])
    plt.xlabel('LD1')
    plt.ylabel('LD2')
    plt.title('LDA é™ç¶­å¾Œç©ºé–“')
    plt.legend(loc='upper right', fancybox=True).get_frame().set_alpha(0.5)
    plt.grid()
    plt.tight_layout()
    plt.show()


def plot_scikit_lda(X, title):
    """
    å‡½æ•¸åŠŸèƒ½ï¼šç¹ªè£½ sklearn è¨ˆç®—çš„ LDA çµæœ
    è¼¸å…¥ï¼š
        Xï¼šé™ç¶­å¾Œè³‡æ–™ï¼ˆ2 ç¶­ï¼‰
        titleï¼šåœ–å½¢æ¨™é¡Œ
    è¼¸å‡ºï¼šmatplotlib è¦–è¦ºåŒ–åœ–å½¢
    """
    ax = plt.subplot(111)
    for label, marker, color in zip(range(1, 4), ('^', 's', 'o'), ('blue', 'red', 'green')):
        plt.scatter(X[y == label, 0], X[y == label, 1] * -1,
                    marker=marker, color=color, alpha=0.5,
                    label=label_dict[label])
    plt.xlabel('LD1')
    plt.ylabel('LD2')
    plt.title(title)
    plt.legend(loc='upper right', fancybox=True).get_frame().set_alpha(0.5)
    plt.grid()
    plt.tight_layout()
    plt.show()
    


if __name__ == '__main__':

    # ========================================
    # Step 1. å®šç¾©æ¬„ä½èˆ‡æ¨™ç±¤å°ç…§è¡¨
    # ========================================
    feature_dict = {i: label for i, label in enumerate([
        'sepal length in cm',
        'sepal width in cm',
        'petal length in cm',
        'petal width in cm'
    ])}

    label_dict = {i: label for i, label in zip(
        range(1, 4), ('Setosa', 'Versicolor', 'Virginica')
    )}

    # ========================================
    # Step 2. è®€å–è³‡æ–™ä¸¦æŒ‡æ´¾æ¬„ä½åç¨±
    # ========================================
    df = pd.read_csv('PCA&LDA/iris.data')
    df.columns = [feature_dict[i] for i in range(4)] + ['class label']
    print("é€™æ˜¯è³‡æ–™å‰äº”ç­†:\n", df.head())

    # ========================================
    # Step 3. åˆ†é›¢ç‰¹å¾µèˆ‡æ¨™ç±¤ï¼Œä¸¦è½‰ç‚ºæ•¸å€¼ç·¨ç¢¼
    # ========================================
    X = df.iloc[:, 0:4].values
    y = df['class label'].values
    y = LabelEncoder().fit_transform(y) + 1  # è½‰ç‚º 1~3 æ•¸å€¼

    # ========================================
    # Step 4. è¨ˆç®—å„é¡åˆ¥çš„å¹³å‡å‘é‡ï¼ˆé¡åˆ¥ä¸­å¿ƒï¼‰
    # ========================================
    mean_vectors = []
    np.set_printoptions(precision=4)  # å°æ•¸é»é¡¯ç¤ºä½æ•¸
    for cl in range(1, 4):
        mean_vec = np.mean(X[y == cl], axis=0)
        mean_vectors.append(mean_vec)
        print(f"é¡åˆ¥ {cl} çš„ç‰¹å¾µå‡å€¼å‘é‡:\n", mean_vec)

    # ========================================
    # Step 5. é¡å…§æ•£åº¦çŸ©é™£ S_W è¨ˆç®—
    # ========================================
    S_W = np.zeros((4, 4))
    for cl, mv in zip(range(1, 4), mean_vectors):
        class_sc_mat = np.zeros((4, 4))
        for row in X[y == cl]:
            row, mv = row.reshape(4, 1), mv.reshape(4, 1)
            class_sc_mat += (row - mv).dot((row - mv).T)
        S_W += class_sc_mat
    print("é¡å…§æ•£åº¦çŸ©é™£ S_W:\n", S_W)

    # ========================================
    # Step 6. é¡é–“æ•£åº¦çŸ©é™£ S_B è¨ˆç®—
    # ========================================
    overall_mean = np.mean(X, axis=0).reshape(4, 1)
    S_B = np.zeros((4, 4))
    for i, mean_vec in enumerate(mean_vectors):
        n = X[y == i + 1].shape[0]
        mean_vec = mean_vec.reshape(4, 1)
        S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)
    print("é¡é–“æ•£åº¦çŸ©é™£ S_B:\n", S_B)

    # ========================================
    # Step 7. å»£ç¾©ç‰¹å¾µå€¼åˆ†è§£ S_W^(-1) S_B
    # ========================================
    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))
    for i in range(len(eig_vals)):
        print(f"\nç‰¹å¾µå‘é‡ {i+1}ï¼š\n", eig_vecs[:, i].real.reshape(4, 1))
        print(f"ç‰¹å¾µå€¼ {i+1}ï¼š{eig_vals[i].real:.4e}")

    # ========================================
    # Step 8. ç‰¹å¾µå€¼æ’åºèˆ‡ç™¾åˆ†æ¯”è²¢ç»
    # ========================================
    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]
    eig_pairs = sorted(eig_pairs, key=lambda x: x[0], reverse=True)

    print("ç‰¹å¾µå€¼æ’åºï¼ˆç”±å¤§åˆ°å°ï¼‰:")
    for val, _ in eig_pairs:
        print(val)

    eigv_sum = sum(eig_vals)
    print("å„ç‰¹å¾µå€¼çš„è§£é‡‹è®Šç•°ç™¾åˆ†æ¯”:")
    for i, pair in enumerate(eig_pairs):
        print(f"ç¬¬ {i+1} ä¸»æˆåˆ†ï¼š{(pair[0]/eigv_sum).real:.2%}")

    # ========================================
    # Step 9. é¸å–å‰ 2 ç¶­ LDA ä¸»æˆåˆ†
    # ========================================
    W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))
    print("æŠ•å½±çŸ©é™£ W:\n", W.real)

    # ========================================
    # Step 10. é™ç¶­è™•ç†ï¼ˆX Ã— Wï¼‰
    # ========================================
    X_lda = X.dot(W)
    print("é™ç¶­å¾Œçš„è³‡æ–™å½¢ç‹€:", X_lda.shape)

    # ========================================
    # Step 11. å‡½æ•¸ï¼šåŸå§‹è³‡æ–™çš„å‰å…©ç¶­è¦–è¦ºåŒ–
    # ========================================

    plot_step_lda()

    # ========================================
    # Step 12. å‡½æ•¸ï¼šLDA é™ç¶­å¾Œè¦–è¦ºåŒ–
    # ========================================

    plot_step_lda2()

    # ========================================
    # Step 13. ä½¿ç”¨ scikit-learn é€²è¡Œ LDA
    # ========================================
    sklearn_lda = LDA(n_components=2)
    X_lda_sklearn = sklearn_lda.fit_transform(X, y)

    # ========================================
    # Step 14. å‡½æ•¸ï¼šsklearn LDA çµæœè¦–è¦ºåŒ–
    # ========================================

    plot_scikit_lda(X_lda_sklearn, title='sklearn å¯¦ä½œçš„ LDA æŠ•å½±çµæœ')
